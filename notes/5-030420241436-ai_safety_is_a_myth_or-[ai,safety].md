Read an article about METR which is non-profit organization for safety-test of AI. It seems like nobody knows how to safety-test AI and METR specifically focusing on whether AI bots can self-replicate itselves and spread through internet. Because, if it can, then it'll be challenging to turn off AI bots. 

But, it's also a pragmatic decision as it requires less specialized expertise than, say, biosecurity testing. Anyways, it's an important topic where 15 different AI companies also commit to have responsibility to test new models for self-replicating techniques.

It sounds like safety-test will become very crucial in the future.

NIST president [Elham Tabassi](https://time.com/collection/time100-ai/6310638/elham-tabassi/) says that community doesn't have an answer for systemic approach at the moment.

It seems like [we have only 5 years left](https://time.com/6564434/connor-leahy-ai-risk-deepfakes/) before AI could pose an existential threat to humanity.

And some company CEO's, like Meta and Cohere, says METR tests are far-fetched. I also agree with this sentiment. I think current LLM modals are far-away from self-replicating. I think danger lies in misusing AI technologies by humans rather than AI itself. People can create synthetic videos, deepfakes, voices etc. and spread them through internet. I think this is the real danger not self-replicating AI bots.

(Nobody Knows How to Safety-Test AI)[https://time.com/6958868/artificial-intelligence-safety-evaluations-risks]
